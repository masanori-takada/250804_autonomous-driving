# PKU Autonomous Driving：実装指示

## 課題の概要

本タスクでは、自動運転車両が取得したセンサーデータを用いて **3D 物体検出** を行います。  
使用するデータセットは、北京大学と Baidu が共同で提供する **PKU‑Baidu Autonomous Driving Dataset** で、以下のようなマルチモーダル センサーデータを含みます。

- **LiDAR 点群（Velodyne HDL‑64E / HDL‑32E）** — 毎秒約 1.3 M 点を 360° で取得し、最大およそ 100 m 先までの高密度 3D 点群を生成  
- **マルチカメラ画像（前後左右 6〜10 台, 1920 × 1080 RGB）** — 広角と望遠を組み合わせた同期撮影で、車両周囲を全方位カバー  
- **GPS / IMU 位置・姿勢トラック（10 Hz）** — RTK‑GPS と慣性計測ユニットを融合し、6 DoF の車両軌跡（x, y, z, roll, pitch, yaw）を提供  
- **カメラ内部・外部パラメータ（完全キャリブレーション行列）** — 焦点距離・主点・歪み係数に加え、LiDAR / 車体座標系への変換行列を含む

これらの情報を融合し、車両・歩行者・交通標識などを 3 次元空間上で高精度に検出することが目的です。センサーフュージョンと空間認識技術が精度向上の鍵となります。

参考リンク:

- [Data ページ](https://www.kaggle.com/competitions/pku-autonomous-driving/data) – データの内容・取得方法  
- [Overview ページ](https://www.kaggle.com/competitions/pku-autonomous-driving/overview) – コンペ全体の概要  

## 評価指標

- モデル性能は **mean Average Precision（mAP）** で評価されます。
  - 3D 空間での位置・サイズ・向きの精度を総合的に評価します。
  - 提出ファイル形式と評価方法の詳細は Kaggle ページを参照してください。  
    ➜ [Evaluation ページ](https://www.kaggle.com/competitions/pku-autonomous-driving/overview/evaluation)

---

## 実装要件

以下の要件に **必ず** 従ってください。

### データの場所
- データセットは `@/data` に格納されています。

### 作成するファイル
- すべての成果物は `@/250804_pku-autonomous-driving` フォルダに格納してください。  
  ※ フォルダ名はプロジェクト方針に合わせて変更しても構いません。
- 提出用ファイル `submission.csv` の生成も同フォルダ内で行ってください。

### 前処理の指示
- 使用するデータ（LiDAR・画像・GPS / IMU など）のフォーマットを十分に理解したうえで、  
  **各モダリティに適した前処理方法を調査・設計し、5 案以上を選定理由とともに提示してください。**
- 処理の妥当性・一貫性を担保し、モデル入力に最適な形でデータを整形してください。

### モデル指定
- `@/data` の内容を確認のうえ、データに適した手法で **2023 年以降に発表された最新モデル 5 案以上** を、選定理由とともに提示してください。  
  **※ 指示があるまで学習・推論は実行しないこと。**
- モデル構造を無断で改変したり、フォールバック予測を行うことは禁止です。
- モデル構造を維持したまま、必要に応じて改善策を検討してください。

### モデルの制約事項
- 過学習を避け、評価対象期間のみ高精度になる「チャンピオンデータ化」を防いでください。  
  **汎化性能** を最重視してください。

### ハイパーパラメータ最適化
- Optuna や GridSearchCV などの **ハイパーパラメータ最適化は実施しない** でください。
- 初期段階での細かなチューニングも不要です。

---

## 出力形式
- Python スクリプト（`.py`）として出力してください。
- 提出ファイル `submission.csv` を必ず生成してください。
